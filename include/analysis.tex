In \sref{modeling}, we formalized the uncertainty affecting electronic systems
and discussed several aspects of such systems along with quantities $\g$ that
the designer is typically interested in analyzing. In the previous section,
\sref{interpolation}, we obtained an efficient interpolation algorithm for
approximating hypothetical multidimensional functions $\f$. We shall now
amalgamate the ideas developed in the aforementioned two sections.

Given an electronic system dependent on a number of uncertain parameters $\vu:
\outcomes \to \real^\nu$, the goal is to analyze a quantity of interest $\g$
representing a certain aspect of the system. For instance, $\vu$ can correspond
to the execution times of the tasks, and $\g$ can correspond the total energy
consumed by the processing elements, as we exemplify in \sref{time} and
\sref{power}. The goal is attained as follows.

1) The parametrization of $\g$ is changed from $\vu$ to \rvs\ $\vz: \outcomes
\to [0, 1]^\nz$ via a suitable transformation $\transformation$; this stage is
described in \sref{parameters}. 2) An interpolant of the resulting composition
$\g \circ \transformation$ is constructed by treating the composition as a
deterministic function $\f$ of $\vz$; this stage is detailed in
\sref{interpolation}. 3) An estimation of the probability distribution of $\g$
is undertaken in the usual sampling-based manner but relying solely on the
constructed interpolant; $\g$ is no longer involved. This last stage boils down
to drawing independent samples from $\distribution_\vz$ and evaluating the
interpolant $\approximation{\l}(\f) \equiv \approximation{\l}(\g \circ
\transformation)$ at those points. Having collected samples of $\g$, other
statistics about $\g$, such as probabilities of particular events, can be
straightforwardly estimated. We do not discuss this estimation stage any further
as it is standard.

There are two aspects concerning the usage of the proposed framework that we
would like to cover in what follows.

\subsection{Expectation and Variance} \slab{moments}
Since the expected value and variance, defined in \eref{expectation} and
\eref{variance}, respectively, usually draw particular attention, we would like
to elaborate on them separately; the interpolants that we construct have
something special to offer in this regard.

As shown in \sref{parameters}, $\g$ can be reparameterized in terms of
independent variables that are uniformly distributed on $[0, 1]^\nz$. This means
that the probability density function of $\vz$ simply equals to one. Therefore,
using \eref{expectation} and \eref{approximation}, we have
\begin{align*}
  \expectation{\g} \approx \expectation{\approximation{\l}(\f)} &= \int_{[0, 1]^\nz} \approximation{\l}(\f)(\vz) \, \d\vz \\
  &= \sum_{\vi \in \lindex_\l} \, \sum_{\vj \in \Delta\oindex_\vi} \surplus(\vx_{\vi\vj}) \, \w_{\vi\vj}
\end{align*}
where
\[
  \w_{\vi\vj} = \int_{[0, 1]^\nz} \e_{\vi\vj}(\vz) \, \d\vz = \prod_{k = 1}^{\nz} \int_0^1 \e_{i_k j_k}(\z_k) \, \d\z_k = \prod_{k = 1}^{\nz} \w_{i_k j_k}.
\]
In the above equation, $\w_{ij}$ is as shown in \eref{volume}. Consequently, we
have obtained an analytical formula for the expected value of $\g$, which does
not require any additional sampling.

Regarding the variance of $\g$, it can be seen in \eref{variance} that the
variance can be assembled from two components: the expected value of $\g$, which
we already have, and the expected value of $\g^2$, which we are missing. The
solution is to let $h = (\g, \g^2)$ be the quantity of interest instead of $\g$.
Then the expected values of both $\g$ and $\g^2$ will be available in analytical
forms, and the variance of $\g$ can be computed using \eref{variance}. This
approach can be generalized to probabilistic moments of higher orders.

\subsection{Multiple Outputs}
The careful reader has noted a problem with the calculation of variance in the
previous subsection: $h$ is vector valued. More generally, the quantity $\g$ in
\sref{modeling} and the function $\f$ in \sref{interpolation} have been depicted
as having one-dimensional codomains. This, however, has been done only for the
sake of clarity. All the mathematics and pseudocodes stay the same for
vector-valued functions. The only except is that, since a surplus
$\surplus(\vx_{\vi\vj})$ naturally inherits the output dimensionality of $\f$,
the operations that involve $\surplus(\vx_{\vi\vj})$ should be adequately
adjusted. If the outputs are on different scales and/or have different accuracy
requirements, one might want to have different $\aerror$ and $\rerror$ in
\eref{stopping-condition} for different outputs. In that case, one also needs to
device a more sensible strategy for scoring collocation nodes in \eref{score}
such as rescaling individual outputs and then calculating the uniform norm
$\norm{\cdot}_\infty$ or $L^2$ norm $\norm{\cdot}_2$. Our code \cite{sources}
has been written with multiple outputs in mind.

To sum up, once an interpolant of $\g$ has been constructed, the probability
distribution of $\g$ is estimated using versatile sampling methods applied to
the interpolant, and this application has a negligible cost. The framework
extends naturally to quantities of interest with multiple outputs, and it
provides analytical formulae for expectations and variances.

Let us remind that the evaluation of $\g$ is an extensive operation. Our
technique is designed to keep this expense as low as possible by choosing the
evaluation points adaptively, which is unlike traditional sampling methods.
Moreover, in contrast to \up{PC} expansions and similar techniques, the proposed
framework is well suited for the nonsmooth response surfaces stemming from the
digital nature of electronic systems.
