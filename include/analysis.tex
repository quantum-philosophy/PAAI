In \sref{modeling}, we formalized the uncertainty affecting electronic systems
and introduced a number of models along with a number of quantities $\g$ that
the designer is typically interested in studying. In the previous section,
\sref{interpolation}, we obtained an efficient interpolation algorithm for
approximating hypothetical multidimensional functions $\f$. Now we shall
amalgamate the ideas developed in the aforementioned two sections.

Given an electronic system dependent on a number of uncertain parameters $\vu:
\outcomes \to \real^\nu$, the goal is to analyze a quantity of interest $\g$
representing a certain aspect of the system. For instance, $\vu$ can correspond
to the execution times of the tasks that the processing elements are to execute,
and $\g$ can correspond the total energy consumed by the processing elements.
The goal is attained as follows. First, the parametrization of $\g$ is changed
from $\vu$ to \rvs\ $\vz: \outcomes \to [0, 1]^\nz$ via a suitable
transformation $\transformation$; this stage is described in
\sref{uncertain-parameters}. Second, an interpolant of the resulting composition
$\g \circ \transformation$ is constructed by treating the composition as a
deterministic function $\f$ of $\vz$; this stage is detailed in
\sref{interpolation}. Third, an estimation of the probability distribution of
$\g$ is undertaken in the usual sampling-based manner but relying solely on the
constructed interpolant; $\g$ is no longer involved. This last stage boils down
to drawing independent samples from $\distribution_\vz$ and evaluating the
interpolant $\smolyak{l}(\f) \equiv \smolyak{l}(\g \circ \transformation)$ at
those points. Needless to say that, having collected samples of $\g$, other
statistics about $\g$, such as probabilities of particular events, can be
straightforwardly estimated. We do not discuss the estimation stage any further
as it is standard. However, there are two other aspects concerning the usage of
the proposed framework that we would like to cover in what follows.

\subsection{Quantities with Multiple Outputs}
The careful reader might have noted that some of the quantities of interest $\g$
mentioned in \sref{modeling} take values in multidimensional spaces; for
instance, power and temperature profiles are in $\real^{\np \times \ns}$, as
shown in \sref{power-consumption} and \sref{heat-dissipation}, respectively.
However, the function $\f$ has been depicted so far as having a one-dimensional
codomain. Then the question is: How to apply the interpolation algorithm when
$\f$ has multiple outputs? The answer is that the mathematics presented in
\sref{interpolation} along with the pseudocodes given in \aref{construct} and
\aref{evaluate} stay the same for vector-valued functions. The only except is
that, since the surpluses $\surplus(\vx_{\vi\vj})$ naturally inherit the
dimensionality of $\f$, the operations that involve $\surplus(\vx_{\vi\vj})$
should be understood elementwise. For example, in \eref{smolyak-hierarchical},
each entry of $\surplus(\vx_{\vi\vj})$ is multiplied by the same basis function
$\e_{\vi\vj}$, which is always scalar valued. Another example is the refinement
criterion given in \eref{serror}. Here the absolute value $|\cdot|$ should be
essentially replaced by the uniform norm $\norm{\cdot}_\infty$. In this last
example, a more preferable alternative is to reside to the $L^2$ norm
$\norm{\cdot}_2$ as the uniform norm might be too harsh when the number of
outputs of $\f$ is large.

\subsection{Expectation and Variance} \slab{probabilistic-moments}
Since the expected value and variance, defined in \eref{expectation} and
\eref{variance}, respectively, usually draw particular attention, we would like
to elaborate on them separately; the interpolants that we construct have
something special to offer in this regard.

As shown in \sref{uncertain-parameters}, $\g$ can be reparameterized in terms of
independent variables that are uniformly distributed on $[0, 1]^\nz$. This means
that the probability density function of $\vz$ simply equals to one. Therefore,
by the law of the unconscious statistician, using \eref{expectation} and
\eref{smolyak-hierarchical}, we have
\begin{align*}
  \expectation{\g} \approx \expectation{\smolyak{l}(\f)} &= \int_{[0, 1]^\nz} \smolyak{l}(\f)(\vz) \, d\vz \\
  &= \sum_{|\vi| \leq l} \, \sum_{\vj \in \Delta\oindex_\vi} \surplus(\vx_{\vi\vj}) \, \w_{\vi\vj}
\end{align*}
where
\[
  \w_{\vi\vj} = \int_{[0, 1]^\nz} \e_{\vi\vj}(\vz) \, d\vz = \prod_{k = 1}^{\nz} \int_0^1 \e_{i_k j_k}(\z_k) \, d\z_k = \prod_{k = 1}^{\nz} \w_{i_k j_k}.
\]
In the above equation, $\w_{ij}$ is as shown in \eref{volume}. Consequently, we
have obtained an analytical formula for the expected value of $\g$, which does
not require any sampling.

Regarding the variance of $\g$, it can be seen in \eref{variance} that the
variance can be assembled from two components: the expected value of $\g$, which
we already have, and the expected value of $\g^2$, which we are missing. The
solution is to let $h = (\g, \g^2)$ be the quantity of interest instead of $\g$.
Then the expected values of both $\g$ and $\g^2$ will be available in analytical
forms, and the variance of $\g$ can be computed using \eref{variance}. This
approach can be generalized to probabilistic moments of higher orders.

To sum up, once an interpolant of $\g$ has been constructed, the probability
distribution of $\g$ is estimated using versatile sampling methods applied to
the interpolant. The framework extends naturally to multi-output quantities of
interest, and it provides analytical formulae for expectations and variances.
