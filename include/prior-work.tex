Sampling methods would be a reasonable solution to probabilistic analysis of
electronic systems if electronic systems were inexpensive to simulate (in terms
of time and other resources). In order to eliminate or reduce the costs
associated with direct sampling, a number of techniques have been introduced.

The techniques are diverse as the area itself and can be classified in many
ways. Two natural distinguishing characteristics originate from the sources of
uncertainty and quantities of interests that the techniques are tailored for. A
technique tries to give a probabilistic characterization of the latter while
taking into account the deteriorating effect of the former. The solution that a
technique proposes in order to tackle the corresponding problem is another
characteristic to pay attention to. A solution can be based on, for instance, a
mathematical framework, statistical method, or computational algorithm. In what
follows, we shall focus on the three aforementioned characteristics.

Let us first discuss analog sources of uncertainty and, more concretely, process
variation as it by far dominates. Circuit-level timing and power analyses under
process variation are undertaken in \cite{bhardwaj2008} by means of polynomial
chaos (\up{PC}) expansions \cite{xiu2010}. The work in \cite{juan2012} models
static steady-state temperature and accounts for process variation by leveraging
the linearity of Gaussian distributions and time-invariant systems. A stochastic
collocation \cite{xiu2010} approach to static steady-state temperature analysis
is given in \cite{lee2013}, which relies on global interpolation using Newton
polynomials. In \cite{ukhov2014}, transient temperature analysis is considered,
and process variation is addressed via \up{PC} expansions. The machinery of
\up{PC} expansions is also utilized in \cite{ukhov2015} in order to model
dynamic steady-state temperature \cite{ukhov2012} and to enhance reliability
models.

Let us now turn to digital sources of uncertainty. In this context, timing
analysis has drawn the major attention \cite{quinton2012}. A seminal work on
response time analysis of periodic tasks with random execution times on
uniprocessors is reported in \cite{diaz2002}. A novel analytical solution to
this problem is given in \cite{tanasa2015}, which makes milder assumptions and
allows for addressing larger, previously unfeasible problems. The framework
proposed in \cite{santinelli2011} facilitates task scheduling by providing
probabilistic bounds on the resource given to a task flow and the resource
needed by that task flow; the approach is based on real-time calculus and is
applicable to electronic systems. Temperature variations due to uncertainties in
timing have also been extensively studied, and the accent has been primarily on
the worst case. An example is the work in \cite{yang2013}, which builds on
real-time calculus and targets the maximum temperature.

Studying the literature on probabilistic analysis of electronic systems, one can
note a pronounced trend: the generality and straightforwardness of sampling
methods tend to be lost. To elaborate, a technique typically: 1) requires
restrictive conditions to be fulfilled such as the absence of dependencies or
the usage of a certain scheduling policy; 2) is tailored to one concrete
quantity such as the response time or maximum temperature; and 3) requires
substantial effort to be deployed.

This trend is understandable: the techniques try to excel by narrowing down the
scope and asking for more. However, one should also keep in mind what is
practical. 1) Despite making mathematics beautiful and analytically solvable,
some of the assumptions often do not hold; the independence assumption is one of
those frequently frowned upon. 2) Having one specific tool for one specific job
might lead to longer adoption times and to the need for more experts. Separation
of concerns is a sustainable strategy as long as tools are composable,
compatible with each other, which, however, is difficult to achieve when
techniques have drastically different foundations. 3) It is often the case that
there has been developed a robust simulator of the quantity of interest for the
utopian deterministic scenario. Switching gears to probabilistic analysis might
mean discarding this battle-tested code all together and implementing something
else from scratch, which is wasteful and not desirable.

Some of the techniques listed earlier in this section, in fact, preserve the
generality and straightforwardness of sampling methods. An example is the
uncertainty analysis presented in \cite{ukhov2015}. The reason is that the
construction of \up{PC} expansions in \cite{ukhov2015} is undertaken by means of
so-called nonintrusive spectral projections \cite{xiu2010}, which do not need to
look inside the ``black box,'' similar to sampling methods. However, as
motivated in \sref{introduction}, nonsmoothness is a serious problem for global
approximation based on polynomials. The convergence of \up{PC} expansions, for
instance, deteriorates substantially in such cases, requiring partitioning of
the stochastic space in order to alleviate the problem. Therefore, it is not
straightforward to apply such techniques as the one given in \cite{ukhov2015} in
the context of digital sources of uncertainty exhibiting nonsmoothness.

To conclude, the available techniques for probabilistic analysis of electronic
systems are restricted in use. Flexible, capable, and easy-to-deploy frameworks
are needed.
