In this section, we evaluate the performance of our framework. Our
implementation is open source and can be found at \cite{sources}, which also
includes the experimental setup along with configuration files and input data.
The experiments discussed below are conducted on a \up{GNU}/Linux machine
equipped with 16 processors Intel Xeon E5520 2.27~\up{GH}z and 24~\up{GB} of
\up{RAM}.

We shall get to grips with $3 \times 2 \times 3 = 18$ uncertainty-quantification
problems. We shall consider three platform sizes $\np$: 2, 4, and 8 processing
elements; two application sizes $\nt$: 10 and 20 tasks; and three quantities of
interest $\g$: the end-to-end delay, total energy consumption, and maximum
temperature defined in \eref{end-to-end-delay}, \eref{total-energy}, and
\eref{maximum-temperature}, respectively. At this point, it might be helpful to
recall the example in \fref{example}.

\subsection{Configuration} \slab{configuration}
\input{include/experimentation/configuration}

\subsection{Discussion}
\input{include/experimentation/discussion}

The message of the above observations is that the designer of an electronic
system can benefit substantially in terms of accuracy per computation time by
switching from direct sampling to the proposed technique. If the designer's
current workhorse is the classical \up{MC} sampling, the switch might lead to
even more dramatic savings than those shown in \fref{results}. Needless to
mention that the gain is especially prominent in situations where the analysis
needs to be performed many times such as when it resides in a design-space
exploration loop.
