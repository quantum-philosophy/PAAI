In this section, we evaluate the performance of our framework. Our
implementation is open source and can be found at \cite{sources}, which also
includes the experimental setup along with configuration files and input data.
The experiments discussed below are conducted on a \up{GNU}/Linux machine
equipped with 16 processors Intel Xeon E5520 2.27~\up{GH}z and 24~\up{GB} of
\up{RAM}.

We shall get to grips with $3 \times 2 \times 3 = 18$ uncertainty-quantification
problems. \updated{We shall consider three platform sizes $\np$: 2, 4, and 8
processing elements; two application sizes $\nt$: 10 and 20 tasks; and three
metrics $\g$: the end-to-end delay, total energy consumption, and maximum
temperature defined in \eref{end-to-end-delay}, \eref{total-energy}, and
\eref{maximum-temperature}, respectively.} At this point, it might be helpful to
recall the example in \fref{example}.

\subsection{Configuration} \slab{configuration}
\input{include/experimentation/configuration}

\subsection{Discussion}
\input{include/experimentation/discussion}

\updated{Lastly, let us consider a real-life example. It means that we need to
couple our framework with a battle-proven simulator used in industry that would
simulate a real application running on a real platform, as orchestrated by our
framework. The scenario is the same as the one in \fref{example} but with an
industrial-standard simulator in place of the left box. Unlike the previous
example, we are not able to obtain the true solution in this case due to the
prohibitive expense of the simulator, which is exactly why our framework is
needed.}

\updated{We use Sniper \cite{carlson2011} and \up{PARSEC} \cite{bienia2011}.}
