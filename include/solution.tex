As noted earlier, making use of an adequate sampling method is a compelling
approach to uncertainty quantification. We would readily apply such a method to
study our quantity of interest $\g$ if only $\g$ had a negligible cost, which it
does not.

Our solution to the above quandary is to construct a light representation of the
heavy $\g$ and study this representation instead of $\g$. The surrogate that we
build is based on adaptive interpolation: $\g$ is evaluated at a number of
strategically chosen collocation nodes, and any other values of $\g$ are
reconstructed on demand (without involving $\g$) using a set of basis functions
mediating between the collected values of $\g$. The benefit of this approach is
in the number of invocations of the quantity of interest $\g$: only a few
evaluations of $\g$ are needed, and the rest of our probabilistic analysis is
powered by the interpolant, which, in contrast to $\g$, has a negligible cost.

Let us delineate the steps involved in the solution process. Recall that $\g$ is
parameterized by the uncertain parameters $\vu$, and these variables are the
only source of randomness. First, we reparameterize $\g$ in terms of an
auxiliary random vector $\vz$ extracted from $\vu$; the necessity of this stage
will become clear later on. Second, we construct an interpolant of $\g$ by
considering $\g$ as a deterministic function of $\vz$ and evaluating $\g$ at a
small set of carefully chosen points. Finally, we estimate the probability
distribution of $\g$ by applying an arbitrary sampling method to the constructed
interpolant of $\g$.

Interpolation of multivariate functions is a challenging task, which should be
approached with a great care. This aspect will be discussed in detail in
\sref{interpolation}. However, before we proceed to interpolation, we first need
to elaborate on the modeling of uncertain parameters and quantities of interest.
