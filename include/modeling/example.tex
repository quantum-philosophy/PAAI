Before we move on to interpolation, let us take a moment and apply the proposed
framework to a small problem in order to get a better feel for how all the
pieces of the framework fit together. A detailed description of our experimental
setup is given in \sref{configuration}; here we give only the bare minimum.

\newcommand{\cores}{\token{PE1} and \token{PE2}}
\newcommand{\tasks}{\token{T1}--\token{T4}}
The addressed problem is depicted in \fref{example}. We consider a platform with
two homogeneous processing elements, \cores, and an application with four tasks,
\tasks. The data dependencies between \tasks\ and their mapping onto \cores\ can
be seen in \fref{example}. This delay is also our quantity of interest $\g$. The
uncertain parameters are the execution times of \token{T2} and \token{T4}
denoted by $\u_1$ and $\u_2$, respectively; the parameters are correlated, and
their marginal distributions are beta distributions. The (deterministic)
execution time of \token{T3} lies in the range of the possible values of the
execution time of \token{T2}, which curtails the impact of $\u_1$ on $\g$ and,
thereby, makes $\g$ nondifferentiable at a cetain location; see
\rref{smoothness}.

The leftmost box in \fref{example} represents a simulator of the system. It
takes an assignment of the execution times of \token{T2} and \token{T3}, $\u_1$
and $\u_2$, and outputs the calculated end-to-end delay. The second box
corresponds to the transformation $\transformation$ described in
\sref{parameters}. It converts the auxiliary variables $\z_1$ and $\z_2$ into
$\u_1$ and $\u_2$ in accordance with $\u_1$ and $\u_2$'s joint distribution. The
third box is our interpolation engine (to be discussed in \sref{interpolation}).
Using 156 strategic invocations of the simulator, the interpolation engine
yields a light surrogate for the simulator (the slim box with rounded corners).
Having obtained such a surrogate, one proceeds to sampling extensively the
surrogate via a sampling method of choice (the rightmost box). The surrogate
takes $\z_1$ and $\z_2$ and returns an approximation of $\g$ at that location.
Recall that the computation cost of this extensive sampling is negligible as
$\g$ is not involved. The samples are then used to compute an estimate of the
distribution of $\g$.

In the graph on the right-hand side of \fref{example}, the blue line shows the
probability density function of $\g$ computed by applying kernel density
estimation to the samples obtained from our surrogate. The yellow line (barely
visible behind the blue line) shows the true density of $\g$; its calculation is
explained in \sref{experimentation}. It can be seen that our solution closely
matches the exact one. In addition, the orange line shows the estimation that
one would get if one sampled $\g$ directly 156 times and used only those samples
in order to calculate the density of $\g$. We see that, for the same budget of
simulations, the solution delivered by our framework is substantially closer to
the true one than the one delivered by na\"{i}ve sampling.
