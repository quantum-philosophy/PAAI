The foremost step of our framework is to change the parameterization of the
problem from the random vector $\vu = (\u_i)_{i = 1}^\nu \sim \distribution_\vu$
to an auxiliary random vector $\vz = (\z_i)_{i = 1}^\nz \sim \distribution_\vz$
such that (i) the support of $\distribution_\vz$ is the unit hypercube $[0,
1]^\nz$, and (ii) $\nz \leq \nu$ has the smallest value needed to retain the
desired level of accuracy. Goal (i) is standardization, which is done primarily
for convenience. Goal (ii) is model-order reduction, which identifies and
eliminates excessive complexity and, hence, speeds up the proposed framework.
The reduction is possible whenever there are dependencies between $(\u_i)$, in
which case one can find such $(\z_i)_{i = 1}^\nz$, $\nz < \nu$, that each $\u_i$
can be recovered from $(\z_i)$. We shall denote the overall transformation by
$\vu = \transformation(\vz)$ where
\begin{equation} \elab{transformation}
  \transformation: \real^\nu \to [0, 1]^\nz.
\end{equation}
Now, for any point $\vz \in [0, 1]^\nz$, we are able to compute the
corresponding $\vu$ and, consequently, the quantity of interest $\g$ as $\g(\vu)
= \g(\transformation(\vz))$; see \sref{problem-formulation}.

We do not impose any restrictions on $\transformation$; however, we would like
to make it specific in order to give a better intuition. To this end, we begin
by assuming that the distribution of $\vu = (\u_i)_{i = 1}^\nu$,
$\distribution_\vu$, is given as a set of marginal distribution functions
\[
  \{ \distribution_{\u_i}: i = 1, \dots, \nu \}
\]
and a copula \cite{nelsen2006}. The copula is a uniform distribution function on
$[0, 1]^\nu$ that captures the dependencies between $(\u_i)$. Furthermore, the
copula is assumed to be a Gaussian copula whose correlation matrix is denote by
$\correlation \in \real^{\nu \times \nu}$.

\begin{remark}
A set of marginals and a copula are sufficient to fully characterize the joint
distribution of $\vu$, $\distribution_\vu$. However, we consider this
distribution to be an approximation rather than the true one. The knowledge of
the true distribution would be an impractical assumption to make. A more
realistic assumption is the availability of the marginals and correlation matrix
of $\vu$, although they are not sufficient to recover the joint of $\vu$ in
general. One prominent solution is to approximate the joint by accompanying the
available marginals by a Gaussian copula constructed based on the available
correlation matrix (see, \eg, \cite{ukhov2014}). This common scenario motivates
our choice of marginals and a Gaussian copula for illustration.
\end{remark}

The number of variables, which is so far $\nu$, has a significant impact on the
computational complexity of the proposed framework. Therefore, an important
component of the framework is model-order reduction, which we shall base on the
discrete Karhunen--Lo\`{e}ve decomposition, also known as the principal
component analysis. We proceed as follows. Since any correlation matrix is real
and symmetric, $\correlation$ admits the eigendecomposition:
\[
  \correlation = \m{V} \m{\Lambda} \m{V}^T
\]
where $\m{V} \in \real^{\nu \times \nu}$ is an orthogonal matrix of the
eigenvectors of $\correlation$, and $\m{\Lambda} = \diag(\lambda_i)_{i = 1}^\nu$
is a diagonal matrix of the eigenvalues of $\correlation$. The eigenvalues
$(\lambda_i)$ correspond to the variances of the corresponding components
revealed by the decomposition. The model-order reduction boils down to selecting
those components whose cumulative contributions to the total variance is above a
certain threshold. Formally, assuming that $(\lambda_i)$ are sorted in the
descending order and given a threshold parameter $\eta \in (0, 1]$, we identify
the smallest $\nz$ such that
\[
  \frac{\sum_{i = 1}^\nz \lambda_i}{\sum_{i = 1}^\nu \lambda_i} \geq \eta.
\]
Denote by $\tilde{\m{V}} \in \real^{\nu \times \nz}$ and $\tilde{\m{\Lambda}}
\in\real^{\nz \times \nz}$ the matrices obtained by truncating $\m{V}$ and
$\m{\Lambda}$, respectively, according to the strategy delineated above.

Now, the transformation $\transformation$ in \eref{transformation} is
\begin{equation} \elab{transformation}
  \vu = \distribution_\vu^{-1} \left( \Phi\left( \tilde{\m{V}} \tilde{\m{\Lambda}}^\frac{1}{2} \, \Phi^{-1}(\vz) \right) \right)
\end{equation}
where the \rvs\ $\vz = (\z_i)_{i = 1}^\nz$ are independent and uniformly
distributed on $[0, 1]$; $\Phi$ and $\Phi^{-1}$ are the distribution function of
the standard Gaussian distribution and its inverse, respectively, which are
applied elementwise; and $\distribution_\vu^{-1} := \distribution_{\u_1}^{-1}
\otimes \cdots \otimes \distribution_{\u_\nz}^{-1}$ are the inverse marginal
distributions of $\vu$, each of which is applied to the corresponding element of
the vector yielded by $\Phi$. To summarize, we have found such a transformation
$\transformation$ and the corresponding distribution $\distribution_\vz$ such
that $\distribution_\vz$ (i) is defined on $[0, 1]^\nz$ and (ii) has the minimal
number of dimensions to preserve $\eta$ portion of the variance. Besides,
$\distribution_\vz$ is trivial to draw samples from.

\begin{remark}
  In the absence of correlations, \eref{transformation} is simply $\vu =
  \distribution_\vu^{-1}(\vz)$, and no model-order reduction is possible.
\end{remark}
