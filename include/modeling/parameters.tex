The foremost step of our framework is to change the parameterization of the
problem from the random vector $\vu = (\u_i)_{i = 1}^\nu \sim \distribution_\vu$
to an auxiliary random vector $\vz = (\z_i)_{i = 1}^\nz \sim \distribution_\vz$
such that: 1) the support of $\distribution_\vz$ is the unit hypercube $[0,
1]^\nz$, and 2) $\nz \leq \nu$ has the smallest value needed to retain the
desired level of accuracy. The first is standardization, which is done primarily
for convenience. The second is model-order reduction, which identifies and
eliminates excessive complexity and, hence, speeds up the solution process. The
reduction is possible whenever there are dependencies between $(\u_i)_{i =
1}^\nu$, in which case one can find such $(\z_i)_{i = 1}^\nz$, $\nz < \nu$, that
each $\u_i$ can be recovered from $(\z_i)_{i = 1}^\nz$. We shall denote the
overall transformation by $\vu = \transformation(\vz)$ where
\begin{equation} \elab{transformation}
  \transformation: \real^\nu \to [0, 1]^\nz.
\end{equation}
For any point $\vz \in [0, 1]^\nz$, we are now able to compute the corresponding
$\vu$ and, consequently, the quantity of interest $\g$ as $(\g \circ
\transformation)(\vz) = \g(\transformation(\vz)) = \g(\vu)$; recall also
\sref{problem}.

Let us consider an example of $\transformation$ in order to understand the
concept better. To this end, we begin by assuming that the distribution of $\vu
= (\u_i)_{i = 1}^\nu$, $\distribution_\vu$, is given as a set of marginal
distribution functions $\{ \distribution_{\u_i} \}_{i = 1}^\nu$ and a copula
\cite{nelsen2006} (see also \sref{preliminaries}). Furthermore, the copula is
assumed to be a Gaussian copula whose correlation matrix is $\correlation \in
\real^{\nu \times \nu}$.

It is important to note the following. A set of marginals and a copula entirely
characterize the joint distribution of $\vu$, that is, $\distribution_\vu$.
However, we consider this distribution as an approximation rather than as the
true one. The knowledge of the true joint would be an impractical assumption to
make. A more realistic assumption is the availability of the marginals and
correlation matrix of $\vu$. In general, these two pieces are not sufficient to
recover the joint of $\vu$; however, the joint can be approximated well by
accompanying the available marginals by a Gaussian copula constructed based on
the available correlation matrix; see \cite{liu1986} and also \cite{ukhov2014}.
Hence, a set of marginals and a Gaussian copula are practical inputs to
probabilistic analysis.

The number of variables, which is so far $\nu$, has a significant impact on the
complexity of the problem at hand. Therefore, an important component of our
framework is model-order reduction, which we shall base on the discrete
Karhunen--Lo\`{e}ve decomposition, also known as the principal component
analysis. We proceed as follows. Since any correlation matrix is real and
symmetric, $\correlation$ admits the eigendecomposition: $\correlation = \m{V}
\m{\Lambda} \m{V}^T$ where $\m{V} \in \real^{\nu \times \nu}$ is an orthogonal
matrix whose columns are the eigenvectors of $\correlation$, and $\m{\Lambda} =
\diag(\lambda_i)_{i = 1}^\nu$ is a diagonal matrix whose diagonal elements are
the eigenvalues of $\correlation$. The eigenvalues $(\lambda_i)_{i = 1}^\nu$
correspond to the variances of the corresponding components revealed by the
decomposition. The model-order reduction boils down to selecting those major
components whose cumulative contributions to the total variance is above a
certain threshold. Formally, assuming that $(\lambda_i)_{i = 1}^\nu$ are sorted
in the descending order and given a threshold $\eta \in (0, 1]$ specifying the
fraction of the total variance to be preserved, we identify the smallest $\nz$
such that
\begin{equation} \elab{reduction}
  \frac{\sum_{i = 1}^\nz \lambda_i}{\sum_{i = 1}^\nu \lambda_i} \geq \eta.
\end{equation}
Denote by $\tilde{\m{V}} \in \real^{\nu \times \nz}$ and $\tilde{\m{\Lambda}}
\in\real^{\nz \times \nz}$ the matrices obtained by truncating $\m{V}$ and
$\m{\Lambda}$, respectively, to preserve only the first $\nz$ components where
$\nz$ is as shown above.

Now, the transformation $\transformation$ in \eref{transformation} is
\begin{equation} \elab{transformation-concrete}
  \vu = \distribution_\vu^{-1} \left( \Phi\left( \tilde{\m{V}} \tilde{\m{\Lambda}}^\frac{1}{2} \, \Phi^{-1}(\vz) \right) \right)
\end{equation}
where the \rvs\ $\vz = (\z_i)_{i = 1}^\nz$ are independent and uniformly
distributed on $[0, 1]^\nz$; $\Phi$ and $\Phi^{-1}$ are the distribution
function of the standard Gaussian distribution and its inverse, respectively,
which are applied elementwise; and $\distribution_\vu^{-1} =
\distribution_{\u_1}^{-1} \times \cdots \times \distribution_{\u_\nz}^{-1}$ is
the Cartesian product of the inverse marginal distributions of $\vu$, which are
applied to the corresponding element of the vector yielded by $\Phi$. In the
absence of correlations, \eref{transformation-concrete} is simply $\vu =
\distribution_\vu^{-1}(\vz)$, and no model-order reduction is possible ($\nu =
\nz$). It might be interesting to note that, using the above transformation, the
distribution $\vu$ gets ``baked'' in $\g$ because it ``disappears'' when $\g$ is
viewed as a function of $\vz$, distributed uniformly.

To summarize, we have found such a transformation $\transformation$ and the
corresponding random vector $\vz \sim \distribution_\vz$ that: 1)
$\distribution_\vz$ is supported by $[0, 1]^\nz$, and 2) $\vz$ has the smallest
number of dimensions $\nz$ needed to preserve $\eta$ portion of the variance.
