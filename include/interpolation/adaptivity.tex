We are ready to discuss adaptivity, and we begin by reiterating briefly out
motivation. Imagine a function that is nearly flat on the first half of $[0, 1]$
and rather irregular on the other. Under these circumstances, it is natural to
expect that, in order to attain the same accuracy, the first half would require
much fewer collocation nodes than the other one; recall the example given in
\fref{motivation}. However, if we followed the construction procedure described
so far, we would not be able to benefit from the peculiar behavior: we would
treat both sides equally and would add all the nodes of each level.

The solution to the above problem is to make the interpolation algorithm
adaptive. First of all, we need to find a way to measure how good our
approximation is at any point in the domain of $\f$. Then, when taking an
interpolation step, instead of bombarding $\f$ with all the nodes that can be
legitimately added to the interpolant at that step, we will only include those
nodes that are located in the regions with poor accuracy as indicated by the
yet-to-be-found accuracy metric. The outlined strategy practically means that
$\Delta\lindex_\l$ and $\Delta\oindex_\vi$ in \eref{smolyak-hierarchical} will
be only subsets of what is possible at step $\l$.

\begin{remark}
It is important to note that, in general, an arbitrary set of nodes does not
correspond to a valid construction. In order to have a valid interpolant, the
corresponding set of level and order indices, $\{ (\vi_k, \vj_k) \}_k$, should
satisfy the so-called admissibility condition \cite{jakeman2012, klimke2006}.
The condition is diligently satisfied in the algorithms presented here.
\end{remark}

Thanks to the hierarchical form obtained in the previous subsection, we already
have a good candidate for the accuracy metric. Recall \eref{surplus}.
Hierarchical surpluses are a natural indicator of the interpolation error: as
noted earlier, they are the difference between the true function and its
approximation at the nodes of the underlying sparse grid. Consequently, after
computing the surpluses corresponding to the nodes of one level, we can recycle
their values in order to decide which of the nodes are to be refined, that is,
which of the nodes of the next level are to be included in the interpolant.

In order to identify ``problematic'' nodes, one can adhere to various
strategies, and ours is as follows. A collocation node $\vx_{\vi\vj}$ is to be
refined if
\begin{equation} \elab{serror}
  \left| \surplus(\vx_{\vi\vj}) \, \w_{\vi\vj} \right| \ge \serror
\end{equation}
where $\surplus(\vx_{\vi\vj})$ and $\w_{\vi\vj}$ are given by \eref{surplus} and
\eref{volume}, respectively, and $\serror$ is a user-defined constant. We refer
to the left-hand side of \eref{serror} as the score of $\vx_{\vi\vj}$ and to
$\serror$ as the score error. The above criterion will be used in our
experiments.

The question now is: How is the refinement of a node undertaken? The refinement
procedure of the Newton--Cotes rule is illustrated in \fref{rule}. The arrows
emerging from a node connect the node with its forward (next-level) neighbors.
The number of such neighbors is two in one dimension and $2 \nin$ in general.
Formally, for a pair $(\vi, \vj)$, the neighbor pairs are
\begin{align}
  \left\{\vphantom{\Big(}\right. \Big( &(i_1, \dots,   i_k + 1, \dots, i_\nin), \elab{neighbors} \\
                                       &(j_1, \dots, 2 j_k + c, \dots, j_\nin) \Big) \left.\vphantom{\Big)} \right\}_{k, c} \nonumber
\end{align}
for $k = 1, \dots, \nin$ and $c \in \{ 0, 2 \}$. Whenever a node is chosen for
refinement, some or all of its neighbors can be added to the interpolant. The
simplest strategy is to include all $2 \nin$ neighbors of each ``problematic''
node, as in \cite{ma2009}.

Equation \eref{serror} gives us local adaptivity. Local adaptivity operates on
the level of individual nodes. However, it is only one of the two types of
adaptivity that we would like to have. The other one is global adaptivity
\cite{klimke2006}. Global adaptivity operates on the level of individual
dimensions. The intuition behind is that, in general, the input variables
manifest themselves (impact $\f$) differently, and the interpolation algorithm
is likely to benefit by prioritizing those variables that the most influential.
An adaptivity strategy that is both local and global is referred to as hybrid,
and this is our goal in this subsection.

Global adaptivity can be attained by revisiting the set of forward neighbors
given in \eref{neighbors}. The set currently contains the neighbors of a node
with respect to all the dimensions.
