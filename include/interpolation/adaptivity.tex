We would like to draw attention to the following two concerns. First of all,
looking at \eref{smolyak-incremental}, it is not clear what the interpolation
level $l$ should be in order to achieve a certain accuracy level. When should
one stop? Secondly, imagine a function that is nearly flat on the first half of
$[0, 1]$ and rather irregular on the other. Under these circumstances, it is
natural to expect that, in order to achieve the same approximating accuracy, the
first half alone would require much fewer collocation nodes than the other.
However, if we followed the construction procedure described so far, we would
not be able to benefit from this anisotropic behavior: we would treat both sides
equally and would add all collocation nodes of each interpolation level we would
step on, which are $\Y^l \setminus \Y^{l - 1} = \bigcup_{|\vi|_1 = l}
\Delta\X^\vi$ for level $l$ as shown in \eref{smolyak-grid-incremental}.

The answer to the two concerns is to make the construction algorithm adaptive.
First, we need to find a way to measure how good our approximation is at any
point in the domain of $\f$. Then, instead of bombarding $\f$ with all the nodes
of each level, we can exclude those nodes that are located in the regions where
the desired level of accuracy has already been reached according to our
accuracy/error metric.

Recall \eref{tensor-surplus-1d}. Hierarchical surpluses are a natural indicator
of the interpolation error: as noted earlier, they are the difference between
the true function and its current approximation at the nodes of the underlying
sparse grid. Consequently, after computing the surpluses corresponding to the
nodes of the current level, we can recycle the surpluses in order to decide
which of the nodes are to be refined. The meaning of refinement will be
discussed in \sref{collocation-nodes}; for the moment, we are only concerned
with identifying ``problematic'' nodes. In this regards, one can adhere to
various strategies \cite{ma2009}; in what follows, we shall discuss one of them.

Let $\error_\absolute$ and $\error_\relative$ be the absolute and relative
errors, respectively, representing the accuracy requirements for the problem at
hand. Then, the node $\vx^\vi_\vj$ on level $l$ is to be refined if
\begin{equation} \elab{absolute-error}
  \left| \surplus_k(\vx^\vi_\vj) \right| > \error_\absolute
\end{equation}
or
\begin{equation} \elab{relative-error}
  \frac{\left| \surplus_k(\vx^\vi_\vj) \right|}{\max_{\vx \in \Z^l} \f_k(\vx) - \min_{\vx \in \Z^l} \f_k(\vx)} > \error_\relative
\end{equation}
for any $k = 1, \dots, \nout$ where $\f_k$ denotes the $k$th output of the
quantity of interest $\f$. In \eref{relative-error}, the set $\Z^l \subseteq
\Y^l$ is the nodes selected so far for interpolation, and the denominator is a
running estimate of the spread of $\f$ over $[0, 1]^\nin$.

To summarize, assuming that collocation points and basis functions have been
carefully chosen, we have obtained an efficient algorithm for high-dimensional
interpolation. The main equations are \eref{smolyak-incremental} and
\eref{tensor-surplus-1d}, which enable to undertake the approximation in an
incremental or hierarchical manner.
