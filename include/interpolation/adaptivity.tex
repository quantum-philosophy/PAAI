Imagine a function that is nearly flat on the first half of $[0, 1]$ and rather
irregular on the other. Under these circumstances, it is natural to expect that,
in order to attain the same accuracy, the first half would require much fewer
collocation nodes than the other one; recall \fref{motivation}. However, if we
followed the construction procedure described so far, we would not be able to
benefit from the peculiar behavior: we would treat both sides equally and would
add all the nodes of each level.

The solution to the above problem is to make the interpolation algorithm
adaptive. To this end, we first need to find a way to measure how good our
approximation is at any point in the domain of $\f$. Then, when refining the
interpolant, instead of bombarding $\f$ with all possible nodes, we will only
choose those that are located in the regions with poor accuracy as indicated by
the yet-to-be-found accuracy metric.

Thanks to the hierarchical form obtained in the previous subsection, we already
have a good material for building an accuracy metric. Recall \eref{surplus}.
Hierarchical surpluses are natural indicators of the interpolation error: they
are the difference between the true function and its approximation at the nodes
of the underlying sparse grid. Hence, they can be recycled in order to
effectively identify ``problematic'' regions. Specifically, we first assign a
score to each node $\vx_{\vi\vj}$ or, equivalently, to each pair of level and
order indices $(\vi, \vj)$:
\begin{equation} \elab{score}
  \score_{\vi\vj} = \left| \surplus(\vx_{\vi\vj}) \, \w_{\vi\vj} \right|
\end{equation}
where $\surplus(\vx_{\vi\vj})$ and $\w_{\vi\vj}$ are given by \eref{surplus} and
\eref{volume}, respectively, and this score is then used in order to guide the
algorithm as we shall explain in the rest of this subsection.

The Smolyak construction in \eref{smolyak-hierarchical} is rewritten as follows:
\begin{equation} \elab{approximation}
  \approximation{\l}(\f) = \approximation{\l-1}(\f) + \sum_{\vi \in \Delta\lindex_\l} \sum_{\vj \in \Delta\oindex_\vi} \surplus(\vx_{\vi\vj}) \,
\e_{\vi\vj}.
\end{equation}
The main different with respect to \eref{smolyak-hierarchical} is that $\l \geq
0$ no longer signifies a Smolyak level but a more abstract interpolation step,
and $\approximation{\l}$ is the interpolant at that step. As always,
$\approximation{-1} = 0$, and the definition of $\surplus$ given in
\eref{surplus} is adjusted accordingly. Lastly, all index sets from now on are
generally subsets of their full-fledged counterparts defined in
\sref{smolyak-algorithm}.

\begin{remark}
All the rules below make sure that the construction in \eref{approximation}
adhere to the same principles as the ones underpinning
\eref{smolyak-hierarchical}. An arbitrary construction is generally invalid.
\end{remark}

Each $\approximation{\l}$ is characterized by a set of level indices
$\lindex_\l$, and each $\vi \in \lindex_\l$ by a set of order indices
$\Delta\oindex_\vi$. At each interpolation step $\l \geq 0$, a single index
$\vi_\l$ is chosen from $\lindex_{\l-1}$ with $\lindex_{-1} = \{ \v{0} \}$. The
chosen index then gives birth to $\Delta\lindex_\l$ and $\{ \Delta\oindex_\vi
\}_{\vi \in \Delta\lindex_\l}$, which shape the increment in the right-hand side
of \eref{approximation}.

The set $\Delta\lindex_\l$ contains the so-called admissible forward neighbors
of $\vi_\l$. Let us now parse the previous sentence. First, the forward
neighbors of an index $\vi$ are given by
\begin{equation} \elab{forward-level-neighbors}
  \left\{ \vi + \v{1}_k: k = 1, \dots, \nin \right\}
\end{equation}
where $\v{1}_k$ is a vector whose elements are zero except for element $k$ equal
to unity. Next, an index $\vi$ is admissible if its inclusion into the index set
in question $\lindex$ keeps the set admissible. Finally, $\lindex$ is admissible
if it satisfies the following condition \cite{klimke2006}:
\begin{equation} \elab{admissibility}
  \vi - \v{1}_k \in \lindex, \text{ for $\vi \in \lindex$ and $k = 1, \dots, \nin$,}
\end{equation}
where, naturally, the cases with $i_k = 0$ need no check.

Now, how is $\vi_\l$ chosen from $\lindex_{\l-1}$ at each iteration of
\eref{approximation}? First of all, each index can be obviously picked at most
once. The rest is resolved by prioritizing the candidates. It is reasonable to
assign a priority to a level index $\vi$ based on the scores of the order
indices associated with it, that is, on the scores of $\oindex_\vi$. We compute
the priority as the average score:
\[
  \score_\vi = \frac{1}{\card{\Delta\oindex_\vi}} \sum_{\vj \in \Delta\oindex_\vi} \score_{\vi\vj}
\]
Consequently, the answer to the above question is that, at each step $\l$, the
index $\vi$ with the highest $\score_\vi$ gets promoted to $\vi_\l$.

Let us now turn to the content of $\Delta\oindex_\vi$ where $\vi = \vi_\l +
\v{1}_k$ for a fixed $k$. It also contains admissible forward neighbors, but
they are order indices, and their construction is drastically different from the
one in \eref{forward-level-neighbors}. Concretely, these indices are identified
by inspecting the backward neighborhood of $\vi$ (analogous to
\eref{forward-level-neighbors}). For each backward neighbor $\hat{\vi} = \vi -
\v{1}_{\hat{k}}$ and each $\vj \in \Delta\oindex_{\hat{\vi}}$, we begin by
checking the following condition:
\[
  \score_{\hat{\vi}\vj} \geq \serror
\]
where $\serror$ is a user-defined constant referred to as the score error. If
the condition holds, the forward neighbors of $\vj$ in dimension $k$ are added
to $\Delta\oindex_\vi$. This procedure for the open Newton--Cotes rule is
illustrated in \fref{grid}. The arrows emerging from a node connect the node
with its forward neighbors. It can be seen that each node has two forward
neighbors (for one dimension); their order indices are
\[
  (j_1, \dots, 2 j_k, \dots, j_\nin) \hspace{1em} \text{and} \hspace{1em} (j_1, \dots, 2 j_k + 2, \dots, j_\nin).
\]
The above refinement procedure is repeated for each index $\vi \in
\Delta\lindex_\l$ with respect to each dimension $k = 1, \dots, \nin$.

The final question is the stopping condition of the approximation process in
\eref{approximation}. Apart from the obvious constraints on the maximum number
of function evaluations and deepness of interpolation (the Smolyak level), we
rely on the following criterion. Assume given two additional constants:
$\aerror$ and $\rerror$ referred to as the absolute and relative error,
respective. Then, the process is terminated as soon as
\begin{equation} \elab{stopping-condition}
  \max_{(\vi, \vj)} \, |\surplus(\vx_{\vi\vj})| \leq \max \left\{ \aerror, \, \rerror (\f_\text{max} - \f_\text{min}) \right\}
\end{equation}
where $\f_\text{min}$ and $\f_\text{max}$ the minimum and maximum observed
values of $\f$, respectively, and the left-hand side is the maximum surplus
whose level index has not been refined yet (considered as $\vi_\l$ at some step
$\l$ in \eref{approximation}). The above criterion is a sound way to curtail the
process it is based on the actual progress.

The adaptivity presented in this subsection is referred to as hybrid as it
exhibits features of both global and local adaptivity. Local adaptivity operates
on the level of individual nodes, and it has already been motivated. Global
adaptivity operates on the level of individual dimensions. The intuition behind
it is that, in general, the input variables manifest themselves (impact $\f$)
differently, and the interpolation algorithm is likely to benefit by
prioritizing those variables that the most influential.
