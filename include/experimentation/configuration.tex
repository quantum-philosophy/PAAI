A platform with $\np$ processing elements and an application with $\nt$ tasks
are generated randomly by the \up{TGFF} tool \cite{dick1998}. The tool generates
$\np$ tables and a directed acyclic graph with $\nt$ nodes. Each table
corresponds to a processing element, and it describes certain properties of the
tasks when they are mapped to that particular processing element. Namely, each
table assigns two numbers to each task: a reference execution time, chosen
uniformly between 10 and 50~ms, and a power consumption, chosen uniformly
between 5 and 25~W. The graph captures data dependencies between the tasks. The
application is scheduled using a list scheduler \cite{adam1974}. The mapping of
the application is fixed and obtained by scheduling the tasks based on their
reference execution times and assigning them to the earliest available
processing elements (a shared ready list).

The construction of thermal \up{RC} circuits needed for temperature analysis is
delegated to the HotSpot tool \cite{skadron2004}. The floorplan of each platform
is a regular grid wherein each processing element occupies $2 \times
2~\text{mm}^2$ on the die. The output of the tool is a pair of a thermal
capacitance matrix $\mC$ and a thermal conductance $\mG$ matrix used in
\eref{thermal-system}. The leakage modeling is based on a linear fit to a data
set of \up{SPICE} simulations of a series of \up{CMOS} invertors
\cite{ukhov2012, liu2007}; see also \cite{ukhov2014}. The time step of power and
temperature profiles is constant and equal to one microsecond; see \sref{power}
and \sref{temperature}.

The uncertain parameters $\vu$ introduced in \sref{problem} are the execution
times of the tasks; see \sref{time}. All other parameters are deterministic.
Targeting the practical scenario described in \sref{parameters}, the marginal
distributions and correlation matrix of $\vu$ are assumed to be available.
Without loss of generality, the marginal of $\u_i$ is a four-parametric beta
distribution $\text{Beta}(\alpha_i, \beta_i, a_i, b_i)$ where $\alpha_i$ and
$\beta_i$ are the shape parameters, and $a_i$ and $b_i$ are the endpoints of the
support. The left $a_i$ and right $b_i$ endpoint are set to 80\% and 120\%,
respectively, of the reference execution time generated by the \up{TGFF} tool as
described earlier. The parameter $\alpha_i$ and $\beta_i$ are set to two and
five, respectively, for all tasks, which skews the corresponding distribution
toward the left endpoint. The execution times of the tasks are correlated based
on the structure of the graph produced by the \up{TGFF} tool: the closer task
$i$ and task $j$ are in the graph as measured by the number of edges between
vertex $i$ and vertex $j$, the stronger $\u_i$ and $\u_j$ are correlated. The
model-order reduction parameter $\eta$ in \eref{reduction} (\sref{parameters})
is set to 0.9, which results in $\nz = 2$ and 3 preserved variables for
applications with $\nt = 10$ and 20 tasks, respectively.

The configuration of the interpolation algorithm (the collocation nodes, basis
functions, and adaptation strategy with stopping conditions) is as described in
\sref{interpolation}. The parameters $\aerror$, $\rerror$, and $\serror$ are
around $10^3$, $10^2$, and $10^4$, respectively, depending on the problem; the
exact values can be found at \cite{sources}, which, again, contains all other
details too.

The performance of our framework with respect to each problem is assessed as
follows. \updated{First, we obtain the ``true'' probability distribution of the
metric in question $\g$ by sampling $\g$ directly and extensively.} Direct
sampling means that samples are drawn from $\g$ itself (not from a surrogate),
and that there is no any intermediate model-order reduction (see
\sref{parameters}). Second, we construct an interpolant for $\g$ and estimate
$\g$'s distribution by sampling the interpolant. In both cases, we draw $10^5$
samples; let us remind, however, that the cost of sampling the interpolant is
practically negligible. \updated{Third, we perform another round of direct
sampling of $\g$, but this time we draw as many samples as many times the metric
was evaluated during the interpolation process.} In each of the three cases, the
sampling is undertaken in accordance with a Sobol sequence, which is a
quasi-random low-discrepancy sequence featuring much better convergence
properties than those of the vanilla Monte-Carlo (\up{MC}) sampling
\cite{joe2008}.

As a result, we obtain three estimates of $\g$'s distribution: reference (the
one considered true), proposed (the one interpolation powered), and direct (the
one equal in terms of the number of $\g$'s evaluations to the proposed
solution). The last two are compared with the first one. For comparing the
proximity between two distributions, we use the well-known Kolmogorov--Smirnov
(\up{KS}) statistic \cite{rao2009}, which is the supremum over the distance
(pointwise) between two empirical distribution functions and, hence, is a rather
unforgiving error indicator.
